{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from layers import architecture, loss_function\n",
    "from utils.cifar_utils import load_data\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/cifar-10-python.tar.gz already exists. Begin extracting...\n",
      "./data/cifar-10-python.tar.gz already exists. Begin extracting...\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train = load_data(mode='train')\n",
    "X_test, y_test = load_data(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "std_image = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train.astype(np.float32) - mean_image.astype(np.float32)) / std_image\n",
    "X_test = (X_test.astype(np.float32) - mean_image) / std_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (50000, 32, 32, 3)\n",
      "Train labels shape:  (50000,)\n",
      "Test data shape (10000, 32, 32, 3)\n",
      "Test labels shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Reshape it to be size NHWC\n",
    "X_train = X_train.reshape([X_train.shape[0],3,32,32]).transpose((0,2,3,1))\n",
    "X_test = X_test.reshape([X_test.shape[0],3,32,32]).transpose((0,2,3,1))\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Test data shape', X_test.shape)\n",
    "print('Test labels shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare tensorflow variables and architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inputs = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "random_rolls = tf.placeholder(dtype=tf.float32, shape=[54])\n",
    "y_inputs = tf.placeholder(dtype=tf.uint8, shape=[None])\n",
    "is_training = tf.placeholder(dtype=tf.bool)\n",
    "y_test_tf = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "test_error_tf = tf.placeholder(dtype=tf.float32, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(X_train)\n",
    "test_size = len(X_test)\n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "boundaries = [int(.5 * epochs * train_size / batch_size),\n",
    "              int(.75 * epochs * train_size / batch_size)]\n",
    "values = [.1, .01, .001]\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "shuffle_indices = np.arange(train_size)\n",
    "\n",
    "labels = tf.one_hot(y_inputs, 10)\n",
    "\n",
    "out = architecture(X_inputs, random_rolls, is_training, strategy='pad',\n",
    "                   scope='stoch_depth', P=0.5, L=54)\n",
    "\n",
    "loss = loss_function(logits=out, labels=labels, weight_decay=weight_decay,\n",
    "                     arch_scope='stoch_depth')\n",
    "training_summary = tf.summary.scalar('train_loss', loss)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "step = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                  momentum,\n",
    "                                  use_nesterov=True).minimize(loss,\n",
    "                                                              global_step=global_step)\n",
    "pred = tf.argmax(out, axis=1)\n",
    "error_num = tf.count_nonzero(pred - y_test_tf, name='error_num')\n",
    "test_summary = tf.summary.scalar('Test_Error', test_error_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create unique log file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"tf_logs/{}/\".format(datetime_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess: \n",
    "    \n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    start_index = 0\n",
    "    iter_number = -1\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        np.random.shuffle(shuffle_indices)\n",
    "        X_train, y_train = X_train[shuffle_indices], y_train[shuffle_indices]\n",
    "        start_index = 0\n",
    "        for _ in range(train_size // batch_size):\n",
    "            iter_number += 1\n",
    "\n",
    "            indices = list(range(start_index, start_index + batch_size))\n",
    "            start_index += batch_size\n",
    "            X_train_batch, y_train_batch = X_train[indices], y_train[indices]\n",
    "\n",
    "            # data augmentation\n",
    "            for inx in range(batch_size):\n",
    "                x_offset = np.random.randint(-4, 4)\n",
    "                y_offset = np.random.randint(-4, 4)\n",
    "                flip_bool = np.random.uniform() > .5\n",
    "                X_train_batch[inx] = np.roll(X_train_batch[inx],\n",
    "                                             (x_offset, y_offset),\n",
    "                                             axis=(0, 1))\n",
    "                if flip_bool:\n",
    "                    X_train_batch[inx] = np.flip(X_train_batch[inx], axis=1)\n",
    "\n",
    "            random_rolls_batch = np.random.uniform(size=54)\n",
    "            _, loss_val, train_summ = sess.run([step, loss, training_summary],\n",
    "                                               feed_dict={X_inputs: X_train_batch,\n",
    "                                                          random_rolls: random_rolls_batch,\n",
    "                                                          y_inputs: y_train_batch,\n",
    "                                                          is_training: True})\n",
    "        writer.add_summary(train_summ, epoch)\n",
    "        start_index_test = 0\n",
    "        scores_list = []\n",
    "        for _ in range(test_size // 100):\n",
    "            indices_test = list(range(start_index_test, start_index_test + 100))\n",
    "            start_index_test += 100\n",
    "\n",
    "            X_test_batch, y_test_batch = X_test[indices_test], y_test[indices_test]\n",
    "\n",
    "            test_batch_accuracy = sess.run(error_num, feed_dict={X_inputs: X_test_batch,\n",
    "                                                                 random_rolls: random_rolls_batch,\n",
    "                                                                 y_test_tf: y_test_batch,\n",
    "                                                                 is_training: False})\n",
    "            scores_list.append(test_batch_accuracy)\n",
    "        test_epoch_accuracy = sum(scores_list) / len(scores_list)\n",
    "        test_summ = sess.run(test_summary, feed_dict={test_error_tf: test_epoch_accuracy})\n",
    "        writer.add_summary(test_summ, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            save_path = saver.save(sess, 'checkpoints/{}/model.ckpt'.format(datetime_str))\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "        print(\"epoch_time =\", epoch_start_time - time.time())\n",
    "            \n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
